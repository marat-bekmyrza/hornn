{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_8_xpmOpxkG",
        "outputId": "401ed2f6-049a-45cb-d2c5-aa0d83ff1eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set seed such that we always get the same dataset\n",
        "# (this is a good idea in general)\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_dataset(num_sequences=2**8):\n",
        "    \"\"\"\n",
        "    Generates a number of sequences as our dataset.\n",
        "    \n",
        "    Args:\n",
        "     `num_sequences`: the number of sequences to be generated.\n",
        "     \n",
        "    Returns a list of sequences.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    \n",
        "    for _ in range(num_sequences): \n",
        "        num_tokens = np.random.randint(1, 12)\n",
        "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
        "        samples.append(sample)\n",
        "        \n",
        "    return samples\n",
        "\n",
        "\n",
        "sequences = generate_dataset()\n",
        "\n",
        "print('A single sample from the generated dataset:')\n",
        "print(sequences[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A single sample from the generated dataset:\n",
            "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agnvY-8pp6fv",
        "outputId": "7ea88474-3582-42a6-b70b-748ef3ebabd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def sequences_to_dicts(sequences):\n",
        "    \"\"\"\n",
        "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
        "    \"\"\"\n",
        "    # A bit of Python-magic to flatten a nested list\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    \n",
        "    # Flatten the dataset\n",
        "    all_words = flatten(sequences)\n",
        "    \n",
        "    # Count number of word occurences\n",
        "    word_count = defaultdict(int)\n",
        "    for word in flatten(sequences):\n",
        "        word_count[word] += 1\n",
        "\n",
        "    # Sort by frequency\n",
        "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
        "\n",
        "    # Create a list of all unique words\n",
        "    unique_words = [item[0] for item in word_count]\n",
        "    \n",
        "    # Add UNK token to list of words\n",
        "    unique_words.append('UNK')\n",
        "\n",
        "    # Count number of sequences and number of unique words\n",
        "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
        "\n",
        "    # Create dictionaries so that we can go from word to index and back\n",
        "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
        "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
        "    idx_to_word = defaultdict(lambda: 'UNK')\n",
        "\n",
        "    # Fill dictionaries\n",
        "    for idx, word in enumerate(unique_words):\n",
        "        word_to_idx[word] = idx\n",
        "        idx_to_word[idx] = word\n",
        "\n",
        "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
        "\n",
        "\n",
        "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
        "\n",
        "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
        "print('The index of \\'b\\' is', word_to_idx['b'])\n",
        "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\n",
        "\n",
        "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
        "    'Consistency error: something went wrong in the conversion.'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 256 sentences and 4 unique tokens in our dataset (including UNK).\n",
            "\n",
            "The index of 'b' is 1\n",
            "The word corresponding to index 1 is 'b'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aarTcKytqjl0",
        "outputId": "13d1066d-7604-4447-85fe-dacd4433f310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_to_idx.items()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('a', 0), ('b', 1), ('EOS', 2), ('UNK', 3)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdkROsuXrp4q",
        "outputId": "d63c58cf-5006-45ae-cda9-13254bcf3610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "idx_to_word.items()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([(0, 'a'), (1, 'b'), (2, 'EOS'), (3, 'UNK')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAeIsO3Hr3b2",
        "outputId": "d6b36846-5f16-43a1-99d7-9652e8aa752e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the size of the dataset\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve inputs and targets at the given index\n",
        "        X = self.inputs[index]\n",
        "        y = self.targets[index]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    \n",
        "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
        "    # Define partition sizes\n",
        "    num_train = int(len(sequences)*p_train)\n",
        "    num_val = int(len(sequences)*p_val)\n",
        "    num_test = int(len(sequences)*p_test)\n",
        "\n",
        "    # Split sequences into partitions\n",
        "    sequences_train = sequences[:num_train]\n",
        "    sequences_val = sequences[num_train:num_train+num_val]\n",
        "    sequences_test = sequences[-num_test:]\n",
        "\n",
        "    def get_inputs_targets_from_sequences(sequences):\n",
        "        # Define empty lists\n",
        "        inputs, targets = [], []\n",
        "        \n",
        "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
        "        # but targets are shifted right by one so that we can predict the next word\n",
        "        for sequence in sequences:\n",
        "            inputs.append(sequence[:-1])\n",
        "            targets.append(sequence[1:])\n",
        "            \n",
        "        return inputs, targets\n",
        "\n",
        "    # Get inputs and targets for each partition\n",
        "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
        "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
        "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
        "\n",
        "    # Create datasets\n",
        "    training_set = dataset_class(inputs_train, targets_train)\n",
        "    validation_set = dataset_class(inputs_val, targets_val)\n",
        "    test_set = dataset_class(inputs_test, targets_test)\n",
        "\n",
        "    return training_set, validation_set, test_set\n",
        "    \n",
        "\n",
        "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
        "\n",
        "print(f'We have {len(training_set)} samples in the training set.')\n",
        "print(f'We have {len(validation_set)} samples in the validation set.')\n",
        "print(f'We have {len(test_set)} samples in the test set.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 204 samples in the training set.\n",
            "We have 25 samples in the validation set.\n",
            "We have 25 samples in the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxMzEAeZsqUt",
        "outputId": "6a038b3b-b506-4271-804c-357dc839d7be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def one_hot_encode(idx, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Args:\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    \"\"\"\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros(vocab_size)\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    one_hot[idx] = 1.0\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def one_hot_encode_sequence(sequence, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
        "    \n",
        "    Args:\n",
        "     `sentence`: a list of words to encode\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "     \n",
        "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
        "    \"\"\"\n",
        "    # Encode each word in the sentence\n",
        "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
        "\n",
        "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
        "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1). # it also can be simply  encoding.reshape(encoding.shape[0], encoding.shape[1])\n",
        "    \n",
        "    return encoding\n",
        "\n",
        "\n",
        "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
        "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
        "\n",
        "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
        "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our one-hot encoding of 'a' has shape (4,).\n",
            "Our one-hot encoding of 'a b' has shape (2, 4, 1).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196UiWMhs9YK",
        "outputId": "2ee3be3e-234f-44e7-9302-d83772ce2feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_word"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkBQ8dfbtWZu",
        "outputId": "5ed79eea-4272-42be-b539-9196844dd3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_sentence"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]],\n",
              "\n",
              "       [[0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKKYTv-3tYpZ",
        "outputId": "6ba4c0dd-bfbe-45a1-939d-3af57ad9b21a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hidden_size = 50 # Number of dimensions in the hidden state\n",
        "vocab_size  = len(word_to_idx) # Size of the vocabulary used\n",
        "\n",
        "def init_orthogonal(param):\n",
        "    \"\"\"\n",
        "    Initializes weight parameters orthogonally.\n",
        "    This is a common initiailization for recurrent neural networks.\n",
        "    \n",
        "    Refer to this paper for an explanation of this initialization:\n",
        "    https://arxiv.org/abs/1312.6120\n",
        "    \"\"\"\n",
        "    if param.ndim < 2:\n",
        "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
        "\n",
        "    rows, cols = param.shape\n",
        "    \n",
        "    new_param = np.random.randn(rows, cols)  # initially normal random\n",
        "    \n",
        "    if rows < cols:\n",
        "        new_param = new_param.T\n",
        "    \n",
        "    # Compute QR factorization\n",
        "    q, r = np.linalg.qr(new_param)\n",
        "    \n",
        "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
        "    d = np.diag(r, 0)\n",
        "    ph = np.sign(d)\n",
        "    q *= ph\n",
        "\n",
        "    if rows < cols:\n",
        "        q = q.T\n",
        "    \n",
        "    new_param = q\n",
        "    \n",
        "    return new_param\n",
        "\n",
        "\n",
        "def init_rnn(hidden_size, vocab_size):\n",
        "    \"\"\"\n",
        "    Initializes our recurrent neural network.\n",
        "    \n",
        "    Args:\n",
        "     `hidden_size`: the dimensions of the hidden state\n",
        "     `vocab_size`: the dimensions of our vocabulary\n",
        "    \"\"\"\n",
        "    # Weight matrix (input to hidden state)\n",
        "    # YOUR CODE HERE!\n",
        "    U = np.zeros((hidden_size, vocab_size))  # U * x -> (hidden_size, vocab_size) * (vocab_size, 1) = (hidden_size, 1)\n",
        "\n",
        "    # Weight matrix (recurrent computation)\n",
        "    # YOUR CODE HERE!\n",
        "    V = np.zeros((hidden_size, hidden_size))  # V * h -> (hidden_size, hidden_size) * (hidden_size, 1) = (hidden_size, 1)\n",
        "    # h -> (hidden_size, 1)\n",
        "\n",
        "    # Weight matrix (hidden state to output)\n",
        "    # YOUR CODE HERE!\n",
        "    W = np.zeros((1, hidden_size))  # W * h -> (1, hidden_size) * (hidden_size, 1) = (1, 1)\n",
        "\n",
        "    # Bias (hidden state)\n",
        "    # YOUR CODE HERE!\n",
        "    b_hidden = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # Bias (output)\n",
        "    # YOUR CODE HERE!\n",
        "    b_out = np.zeros((1, 1))\n",
        "    \n",
        "    # Initialize weights\n",
        "    U = init_orthogonal(U)\n",
        "    V = init_orthogonal(V)\n",
        "    W = init_orthogonal(W)\n",
        "    \n",
        "    # Return parameters as a tuple\n",
        "    return U, V, W, b_hidden, b_out\n",
        "\n",
        "\n",
        "params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size)\n",
        "print('U:', params[0].shape)\n",
        "print('V:', params[1].shape)\n",
        "print('W:', params[2].shape)\n",
        "print('b_hidden:', params[3].shape)\n",
        "print('b_out:', params[4].shape)\n",
        "\n",
        "for param in params:\n",
        "    assert param.ndim == 2, \\\n",
        "        'all parameters should be 2-dimensional '\\\n",
        "        '(hint: a dimension can simply have size 1)'"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U: (50, 4)\n",
            "V: (50, 50)\n",
            "W: (1, 50)\n",
            "b_hidden: (50, 1)\n",
            "b_out: (1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuDSlOyWwkHY"
      },
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise sigmoid activation function for an array x.\n",
        "\n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = 1 / (1 + np.exp(-x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        return f * (1 - f)\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqS8dG2MytgA"
      },
      "source": [
        "def tanh(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise tanh activation function for an array x.\n",
        "\n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        return 1-f**2\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjYrhRvbyu8L"
      },
      "source": [
        "def softmax(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the softmax for an array x.\n",
        "    \n",
        "    Args:\n",
        "     `x`: the array where the function is applied\n",
        "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    x_safe = x + 1e-12\n",
        "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
        "    \n",
        "    if derivative: # Return the derivative of the function evaluated at x\n",
        "        pass # We will not need this one\n",
        "    else: # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUiJhk5qyxqO",
        "outputId": "9258678f-5f49-4bc1-b636-f0c4bd8bbb0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def forward_pass(inputs, hidden_state, params):\n",
        "    \"\"\"\n",
        "    Computes the forward pass of a vanilla RNN.\n",
        "    \n",
        "    Args:\n",
        "     `inputs`: sequence of inputs to be processed\n",
        "     `hidden_state`: an already initialized hidden state\n",
        "     `params`: the parameters of the RNN\n",
        "    \"\"\"\n",
        "    # First we unpack our parameters\n",
        "    U, V, W, b_hidden, b_out = params\n",
        "    \n",
        "    # Create a list to store outputs and hidden states\n",
        "    outputs, hidden_states = [], []\n",
        "    \n",
        "    # For each element in input sequence\n",
        "    for t in range(len(inputs)):\n",
        "\n",
        "        # Compute new hidden state\n",
        "        # YOUR CODE HERE!\n",
        "        hidden_state = tanh(np.dot(U, inputs[t]) + np.dot(V, hidden_state) + b_hidden)\n",
        "\n",
        "        # Compute output\n",
        "        # YOUR CODE HERE!\n",
        "        out = softmax(np.dot(W, hidden_state))\n",
        "        \n",
        "        # Save results and continue\n",
        "        outputs.append(out)\n",
        "        hidden_states.append(hidden_state.copy())\n",
        "    \n",
        "    return outputs, hidden_states\n",
        "\n",
        "\n",
        "# Get first sequence in training set\n",
        "test_input_sequence, test_target_sequence = training_set[0]\n",
        "\n",
        "# One-hot encode input and target sequence\n",
        "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size)\n",
        "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size)\n",
        "\n",
        "# Initialize hidden state as zeros\n",
        "hidden_state = np.zeros((hidden_size, 1))\n",
        "\n",
        "# Now let's try out our new function\n",
        "outputs, hidden_states = forward_pass(test_input, hidden_state, params)\n",
        "\n",
        "print('Input sequence:')\n",
        "print(test_input_sequence)\n",
        "\n",
        "print('\\nTarget sequence:')\n",
        "print(test_target_sequence)\n",
        "\n",
        "print('\\nPredicted sequence:')\n",
        "print([idx_to_word[np.argmax(output)] for output in outputs])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence:\n",
            "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
            "\n",
            "Target sequence:\n",
            "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
            "\n",
            "Predicted sequence:\n",
            "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJZG1pU5zjbT",
        "outputId": "588eceb0-bdf9-424a-aabe-6b208dfd50d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "def clip_gradient_norm(grads, max_norm=0.25):\n",
        "    \"\"\"\n",
        "    Clips gradients to have a maximum norm of `max_norm`.\n",
        "    This is to prevent the exploding gradients problem.\n",
        "    \"\"\" \n",
        "    # Set the maximum of the norm to be of type float\n",
        "    max_norm = float(max_norm)\n",
        "    total_norm = 0\n",
        "    \n",
        "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
        "    for grad in grads:\n",
        "        grad_norm = np.sum(np.power(grad, 2))\n",
        "        total_norm += grad_norm\n",
        "    \n",
        "    total_norm = np.sqrt(total_norm)\n",
        "    \n",
        "    # Calculate clipping coeficient\n",
        "    clip_coef = max_norm / (total_norm + 1e-6)\n",
        "    \n",
        "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
        "    if clip_coef < 1:\n",
        "        for grad in grads:\n",
        "            grad *= clip_coef\n",
        "    \n",
        "    return grads\n",
        "\n",
        "\n",
        "def backward_pass(inputs, outputs, hidden_states, targets, params):\n",
        "    \"\"\"\n",
        "    Computes the backward pass of a vanilla RNN.\n",
        "    \n",
        "    Args:\n",
        "     `inputs`: sequence of inputs to be processed\n",
        "     `outputs`: sequence of outputs from the forward pass\n",
        "     `hidden_states`: sequence of hidden_states from the forward pass\n",
        "     `targets`: sequence of targets\n",
        "     `params`: the parameters of the RNN\n",
        "    \"\"\"\n",
        "    # First we unpack our parameters\n",
        "    U, V, W, b_hidden, b_out = params\n",
        "    \n",
        "    # Initialize gradients as zero\n",
        "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
        "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
        "    \n",
        "    # Keep track of hidden state derivative and loss\n",
        "    d_h_next = np.zeros_like(hidden_states[0])\n",
        "    loss = 0\n",
        "    \n",
        "    # For each element in output sequence\n",
        "    # NB: We iterate backwards s.t. t = N, N-1, ... 1, 0\n",
        "    for t in reversed(range(len(outputs))):\n",
        "\n",
        "        # Compute cross-entropy loss (as a scalar)\n",
        "        # When taking logarithms, it's a good idea to add a small constant (e.g. 1e-9)\n",
        "        # YOUR CODE HERE!\n",
        "        loss += \n",
        "        \n",
        "        # Backpropagate into output (derivative of cross-entropy)\n",
        "        # If you're confused about this step, see this link for an explanation:\n",
        "        # http://cs231n.github.io/neural-networks-case-study/#grad\n",
        "        d_o = outputs[t].copy()\n",
        "        d_o[np.argmax(targets[t])] -= 1\n",
        "        \n",
        "        # Backpropagate into W\n",
        "        # YOUR CODE HERE!\n",
        "        d_W += \n",
        "        d_b_out += d_o\n",
        "        \n",
        "        # Backpropagate into h\n",
        "        d_h = np.dot(W.T, d_o) + d_h_next\n",
        "        \n",
        "        # Backpropagate through non-linearity\n",
        "        # (we assume tanh is used here)\n",
        "        d_f = (1 - hidden_states[t]**2) * d_h\n",
        "        d_b_hidden += d_f\n",
        "        \n",
        "        # Backpropagate into U\n",
        "        # YOUR CODE HERE!\n",
        "        d_U += \n",
        "        \n",
        "        # Backpropagate into V\n",
        "        # YOUR CODE HERE!\n",
        "        d_V += \n",
        "        d_h_next = np.dot(V.T, d_f)\n",
        "    \n",
        "    # Pack gradients\n",
        "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
        "    \n",
        "    # Clip gradients\n",
        "    grads = clip_gradient_norm(grads)\n",
        "    \n",
        "    return loss, grads\n",
        "\n",
        "\n",
        "loss, grads = backward_pass(test_input, outputs, hidden_states, test_target, params)\n",
        "\n",
        "print('We get a loss of:')\n",
        "print(loss)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-8df538b537a4>\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    loss +=\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uufs8heozlja"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}