{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk8Jgz4HUeg_"
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1619470013102,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "rb-Ch4I7Uh2u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204876,
     "status": "ok",
     "timestamp": 1619477065493,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "jEG2-xA1UF4q",
    "outputId": "5b141305-9b0e-4f6a-bdfc-cace54bb8c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\n",
      "unzip:  cannot find or open glove.6B.zip, glove.6B.zip.zip or glove.6B.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!wget -P /data/imdb http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6382,
     "status": "ok",
     "timestamp": 1619477157844,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "5ucFyAseWDr7",
    "outputId": "9f17de51-30ab-4f9b-d9df-67b71cd6fb7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('data/imdb/glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQtbTlKH7vKZ"
   },
   "source": [
    "## **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10391,
     "status": "ok",
     "timestamp": 1619477267016,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "eQyBMa1o0WIy"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 7408,
     "status": "ok",
     "timestamp": 1619477267022,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "UDou0nXj0bUN"
   },
   "outputs": [],
   "source": [
    "word2index = imdb.get_word_index()\n",
    "index2word = dict([(value, key) for (key, value) in word2index.items()])\n",
    "example_review = ' '.join([index2word.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 5684,
     "status": "ok",
     "timestamp": 1619477267023,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "3ji3f5350ebf",
    "outputId": "31cb415f-c623-41ea-da32-1b527f0b6f23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1619478895249,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "XGY0vEZnXIAu",
    "outputId": "bcb2eb39-68f0-45d0-8b9d-394ffea2c923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 60150 words (28434 misses)\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "inp_size = 50\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = {}\n",
    "embedding_matrix[0] = np.zeros((inp_size,1))\n",
    "for word, i in word2index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector.reshape((inp_size, 1))\n",
    "        hits += 1\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros((inp_size,1))\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2091,
     "status": "ok",
     "timestamp": 1619477542888,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "6XX6L-a6Y-4Z"
   },
   "outputs": [],
   "source": [
    "def filter(datax, datay, seq_len=300):\n",
    "    filteredx, filteredy = [], []\n",
    "    for i in range(len(datax)):\n",
    "        if len(datax[i]) <= seq_len:\n",
    "            filteredx.append(datax[i])\n",
    "            filteredy.append(datay[i])\n",
    "    return filteredx, filteredy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1685,
     "status": "ok",
     "timestamp": 1619483959989,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "YZ1Queeb3l6r",
    "outputId": "bc72b97a-b66e-4c90-b75f-d98b6cbc0b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_train shape: (14323, 200)\n",
      "input_test shape: (14732, 200)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "\n",
    "trainx, trainy = filter(train_data, train_labels, seq_len)\n",
    "x_train = sequence.pad_sequences(trainx, maxlen=seq_len, padding='pre')\n",
    "y_train = np.asarray(trainy).astype('float32')\n",
    "\n",
    "testx, testy = filter(test_data, test_labels, seq_len)\n",
    "x_test = sequence.pad_sequences(testx, maxlen=seq_len, padding='pre')\n",
    "y_test = np.asarray(testy).astype('float32')\n",
    "\n",
    "print('input_train shape:', np.array(x_train).shape)\n",
    "print('input_test shape:', np.array(x_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1619483965752,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "oqfzc5iNAcaS"
   },
   "outputs": [],
   "source": [
    "def data_generator(datax, datay, batch_size=128):\n",
    "    \"\"\"\n",
    "    generates 1 batch of inputs\n",
    "    1 batch = 128 sequences\n",
    "    1 sequence = 100 time points / samples\n",
    "    1 sample = 10,000 features\n",
    "    inputs shape: (128, 100, 10000) or (batch_size, seq_len, num_features)\n",
    "    targets shape: (128, 1), 1 target value for each sequence, many-to-one architecture\n",
    "    \"\"\"\n",
    "    inputs, targets = {}, {}\n",
    "    start = 0 # where to start drawing inputs, incremented by batch size * seq_len\n",
    "    while True:\n",
    "        if start + batch_size > len(datax):\n",
    "            start = 0\n",
    "        stop = start + batch_size\n",
    "        indices = np.arange(start, stop)\n",
    "        for i in range(len(indices)):\n",
    "            inputs[i] = {}\n",
    "            sequence = datax[indices[i]]\n",
    "            for j in range(len(sequence)):\n",
    "                inputs[i][j] = embedding_matrix[sequence[j]]\n",
    "            targets[i] = datay[indices[i]]\n",
    "        start = stop\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1619483965753,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "JIr1o6vpBOoI",
    "outputId": "3f2ed7cc-a603-4e51-c155-797d52ae2eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 1920\n",
      "val len: 640\n",
      "test len: 640\n"
     ]
    }
   ],
   "source": [
    "b_size = 64\n",
    "train_steps, val_steps, test_steps = 30, 10, 10\n",
    "print('train len:', b_size*train_steps)\n",
    "print('val len:', b_size*val_steps)\n",
    "print('test len:', b_size*test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "executionInfo": {
     "elapsed": 674,
     "status": "ok",
     "timestamp": 1619483965754,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "fq0-esOK3pMp"
   },
   "outputs": [],
   "source": [
    "train_gen = data_generator(x_train[:b_size*train_steps], \n",
    "                     y_train[:b_size*train_steps], batch_size=b_size)\n",
    "val_gen = data_generator(x_test[:b_size*val_steps], \n",
    "                   y_test[:b_size*val_steps], batch_size=b_size)\n",
    "test_gen = data_generator(x_test[b_size*val_steps:b_size*(val_steps+test_steps)], \n",
    "                    y_test[b_size*val_steps:b_size*(val_steps+test_steps)], batch_size=b_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIMd8wRj70Vn"
   },
   "source": [
    "## **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "executionInfo": {
     "elapsed": 1424,
     "status": "ok",
     "timestamp": 1619478512737,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "9xgPK12j1lUX"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):                                     \n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1619478512738,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "iTdMk3bv3gll"
   },
   "outputs": [],
   "source": [
    "def gradient_clip(grads, max_norm, order=1):\n",
    "    total_norm = 0.0\n",
    "    if order > 1:\n",
    "        for i in range(order):\n",
    "            total_norm += np.linalg.norm(grads[0][-i-1])\n",
    "    else:\n",
    "        total_norm += np.linalg.norm(grads[0])\n",
    "    for i in range(1, len(grads[1:])):\n",
    "        total_norm += np.linalg.norm(grads[i])\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    if total_norm > max_norm:\n",
    "        if order > 1:\n",
    "            for i in range(order):\n",
    "                grads[0][-i-1] *= max_norm / total_norm\n",
    "        else:\n",
    "            grads[0] *= max_norm / total_norm\n",
    "        for i in range(1, len(grads[1:])):\n",
    "            grads[i] *= max_norm / total_norm\n",
    "    \"\"\"\n",
    "    # andrej\n",
    "    for i in range(len(grads)):\n",
    "        np.clip(grads[i], -5, 5, out=grads[i]) # clip to mitigate exploding gradients\n",
    "    \"\"\"\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "executionInfo": {
     "elapsed": 1422,
     "status": "ok",
     "timestamp": 1619478512739,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "ta0JjlsxslDt"
   },
   "outputs": [],
   "source": [
    "def find_acc(outputs, targets):\n",
    "    acc = 0.0\n",
    "    for n in range(len(outputs)):\n",
    "        acc += np.rint(outputs[n]) == targets[n]\n",
    "    acc /= len(outputs)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1619478512741,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "2wqjeeynsMfo"
   },
   "outputs": [],
   "source": [
    "def find_loss(outputs, targets):\n",
    "    loss = 0.0\n",
    "    for n in range(len(outputs)):\n",
    "        loss += -(targets[n]*np.log(outputs[n]) + (1 - targets[n])*np.log(1 - outputs[n]))\n",
    "    loss /= len(outputs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1619478512742,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "YXlAMq3Ps0VN"
   },
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr, order=1):\n",
    "    if order > 1:\n",
    "        for i in range(order):\n",
    "            params[0][-i-1] -= grads[0][-i-1] * lr\n",
    "    else:\n",
    "        params[0] -= grads[0] * lr\n",
    "    for i in range(1, len(params[1:])):\n",
    "        params[i] -= grads[i] * lr\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6825TQW74kX"
   },
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1619483654244,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "-MSiZ6gV3kzD"
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, hidden_size, input_size, out_size):\n",
    "        V = np.random.randn(hidden_size, hidden_size)*0.1 # W_hh\n",
    "        U = np.random.randn(hidden_size, input_size)*0.1  # W_hx\n",
    "        W = np.random.randn(out_size, hidden_size)*0.1    # W_hy\n",
    "        b_hidden = np.random.randn(hidden_size, 1)              # b_h\n",
    "        b_out = np.random.randn(out_size, 1)                    # b_y\n",
    "        self.params = [V, U, W, b_hidden, b_out]\n",
    "        self.best = [V, U, W, b_hidden, b_out]\n",
    "    \n",
    "    def forward(self, inputs, hidden_state, best=False):\n",
    "        if best: \n",
    "            V, U, W, b_hidden, b_out = self.best    \n",
    "        else: \n",
    "            V, U, W, b_hidden, b_out = self.params\n",
    "        outputs, hidden_states = {}, {}\n",
    "        for n in range(len(inputs)):\n",
    "            hidden_states[n] = {}\n",
    "            hidden_states[n][-1] = np.copy(hidden_state)\n",
    "            for t in range(len(inputs[n])):\n",
    "                hidden_states[n][t] = np.tanh(np.dot(U, inputs[n][t]) + np.dot(V, hidden_states[n][t-1]) + b_hidden)\n",
    "            outputs[n] = sigmoid(np.copy((np.dot(W, hidden_states[n][t]) + b_out).item()))\n",
    "        return outputs, hidden_states\n",
    "\n",
    "    def backward(self, inputs, outputs, hidden_states, targets, clip_norm):\n",
    "        V, U, W, b_hidden, b_out = self.params\n",
    "        dV, dU, dW = np.zeros_like(V), np.zeros_like(U), np.zeros_like(W)\n",
    "        db_hidden, db_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "        \n",
    "        loss = 0.0\n",
    "        N, T = len(inputs), len(inputs[0])\n",
    "        for n in range(N): # iterate over sequences in a batch, 128\n",
    "            loss += -(targets[n]*np.log(outputs[n]) + (1 - targets[n])*np.log(1 - outputs[n]))\n",
    "            do = outputs[n] - targets[n]\n",
    "            dW += np.dot(do, hidden_states[n][T-1].T)\n",
    "            db_out += do\n",
    "            dh = np.dot(W.T, do)\n",
    "            for t in reversed(range(T)):\n",
    "                df = (1 - hidden_states[n][t]*hidden_states[n][t]) * dh\n",
    "                db_hidden += df\n",
    "                dU += np.dot(df, inputs[n][t].T)\n",
    "                dV += np.dot(df, hidden_states[n][t-1].T)\n",
    "                dh = np.dot(V.T, df)\n",
    "                \n",
    "        loss /= N\n",
    "        dV, dU, dW, db_hidden, db_out = dV/N, dU/N, dW/N, db_hidden/N, db_out/N\n",
    "        grads = [dV, dU, dW, db_hidden, db_out]\n",
    "        grads = gradient_clip(grads, clip_norm)\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, train_set, valid_set, hidden_state, num_epochs, lr, clip_norm, train_steps=100, val_steps=100):\n",
    "        training_loss, validation_loss = [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_training_loss = 0.0\n",
    "            epoch_validation_loss = 0.0\n",
    "            min_valid_loss = 1e6\n",
    "\n",
    "            for train_step in range(train_steps):\n",
    "                inputs, targets = next(train_set)\n",
    "                outputs, hidden_states = self.forward(inputs, hidden_state)\n",
    "                loss, grads = self.backward(inputs, outputs, hidden_states, targets, clip_norm)\n",
    "                self.params = update_parameters(self.params, grads, lr)\n",
    "                epoch_training_loss += loss\n",
    "\n",
    "            for val_step in range(val_steps):\n",
    "                inputs, targets = next(valid_set)\n",
    "                outputs, _ = self.forward(inputs, hidden_state)\n",
    "                loss = find_loss(outputs, targets)\n",
    "                if loss < min_valid_loss:\n",
    "                    self.best = self.params.copy()\n",
    "                    min_valid_loss = loss\n",
    "                epoch_validation_loss += loss\n",
    "\n",
    "            training_loss.append(epoch_training_loss/train_steps)\n",
    "            validation_loss.append(epoch_validation_loss/val_steps)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(f'Epoch {epoch}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "        return training_loss, validation_loss\n",
    "\n",
    "    def test(self, test_set, hidden_state, test_steps=100):\n",
    "        test_loss, test_acc = 0.0, 0.0\n",
    "        for test_step in range(test_steps):\n",
    "            inputs, targets = next(test_set)\n",
    "            outputs, _ = self.forward(inputs, hidden_state, best=True)\n",
    "            test_loss += find_loss(outputs, targets)\n",
    "            test_acc += find_acc(outputs, targets)\n",
    "        test_loss /= test_steps\n",
    "        test_acc /= test_steps\n",
    "        print(\"Test loss: \", test_loss)\n",
    "        print(\"Test acc: \", test_acc)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 232361,
     "status": "error",
     "timestamp": 1619483886923,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "ZFTfWowUCm21",
    "outputId": "82d99604-6d1e-4ce1-8435-76c7b90921a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 0\n",
      "Epoch 0, training loss: 0.7041292843009869, validation loss: 0.6956561066268989\n",
      "Epoch 1, training loss: 0.6943755685579942, validation loss: 0.6955373235939893\n",
      "Epoch 2, training loss: 0.6943560163845611, validation loss: 0.695539225683255\n",
      "Epoch 3, training loss: 0.6943441671218752, validation loss: 0.6955415209174542\n",
      "Epoch 4, training loss: 0.6943325361760658, validation loss: 0.695543828124069\n",
      "Epoch 5, training loss: 0.6943210907143421, validation loss: 0.6955461438939571\n",
      "Epoch 6, training loss: 0.6943098218157912, validation loss: 0.695548466266291\n",
      "Epoch 7, training loss: 0.694298721134911, validation loss: 0.6955507934941293\n",
      "Epoch 8, training loss: 0.6942877807944864, validation loss: 0.6955531240259145\n",
      "Epoch 9, training loss: 0.694276993355988, validation loss: 0.6955554564921139\n",
      "Epoch 10, training loss: 0.6942663517921929, validation loss: 0.6955577896929163\n",
      "Epoch 11, training loss: 0.6942558494616208, validation loss: 0.6955601225868572\n",
      "Epoch 12, training loss: 0.694245480084659, validation loss: 0.6955624542802811\n",
      "Epoch 13, training loss: 0.6942352377212456, validation loss: 0.6955647840175545\n",
      "Epoch 14, training loss: 0.6942251167499941, validation loss: 0.6955671111719668\n",
      "Epoch 15, training loss: 0.6942151118486636, validation loss: 0.6955694352372602\n",
      "Epoch 16, training loss: 0.6942052179758738, validation loss: 0.6955717558196994\n",
      "Epoch 17, training loss: 0.6941954303539776, validation loss: 0.6955740726306723\n",
      "Epoch 18, training loss: 0.6941857444530126, validation loss: 0.6955763854797457\n",
      "Epoch 19, training loss: 0.6941761559756584, validation loss: 0.6955786942681453\n",
      "Epoch 20, training loss: 0.6941666608431255, validation loss: 0.6955809989826213\n",
      "Epoch 21, training loss: 0.6941572551819208, validation loss: 0.6955832996896719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-7f28d8058940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                             \u001b[0mhidden_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                             \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_clip_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                             train_steps=train_steps, val_steps=val_steps)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtest_loss_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-309-02367a4978e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_set, valid_set, hidden_state, num_epochs, lr, clip_norm, train_steps, val_steps)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-309-02367a4978e1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden_state, best)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Train Vanilla RNN ###\n",
    "# Hyperparameters\n",
    "hidden_size = 16\n",
    "gradient_clip_norm = 1.0\n",
    "learning_rate = 0.1\n",
    "num_epochs = 30\n",
    "\n",
    "# experiments with Vanilla RNN\n",
    "num_exp = 3 # number of experiments\n",
    "test_loss_results, test_acc_results = [], []\n",
    "for n in range(num_exp):\n",
    "    print('Experiment:', n)\n",
    "    rnn_model = RNN(hidden_size=hidden_size, input_size=50, out_size=1)\n",
    "    hidden_state = np.zeros((hidden_size, 1))  # Initial hidden_state\n",
    "    train_loss, valid_loss = rnn_model.train(train_set=train_gen, valid_set=val_gen, \n",
    "                                            hidden_state=hidden_state, num_epochs=num_epochs, \n",
    "                                            lr=learning_rate, clip_norm=gradient_clip_norm, \n",
    "                                            train_steps=train_steps, val_steps=val_steps)\n",
    "    test_loss, test_acc = rnn_model.test(test_set=test_gen, hidden_state=hidden_state, test_steps=test_steps)\n",
    "    test_loss_results.append(test_loss)\n",
    "    test_acc_results.append(test_acc)\n",
    "    # Plot training and validation loss\n",
    "    epoch = np.arange(len(train_loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_loss, 'r', label='Training loss',)\n",
    "    plt.plot(epoch, valid_loss, 'b', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "    plt.savefig(f'imdb_{seq_len}_exp_{n}_rnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435595,
     "status": "ok",
     "timestamp": 1619458011313,
     "user": {
      "displayName": "Marat Bekmyrza",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib-0tD7WjH3aGtX_5Rj6vIsFhVEXlrGvZoOgMH=s64",
      "userId": "16583475954751235795"
     },
     "user_tz": -360
    },
    "id": "iNbAaoArGsYl",
    "outputId": "32ad3e21-4d3c-4056-a328-7da0da33c473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average imdb test loss: seq_len: 50, 0.7562751892290948\n",
      "Average imdb test acc: seq_len: 50, 0.56875\n"
     ]
    }
   ],
   "source": [
    "print(f'Average imdb test loss: seq_len: {seq_len}, {np.average(test_loss_results)}')\n",
    "print(f'Average imdb test acc: seq_len: {seq_len}, {np.average(test_acc_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYFOJySjLNK_"
   },
   "source": [
    "## HORNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML2XN1j33rx3"
   },
   "outputs": [],
   "source": [
    "class HORNN():\n",
    "    def __init__(self, hidden_size, input_size, out_size, order):\n",
    "        self.order = order\n",
    "        V = {}\n",
    "        for i in range(self.order):\n",
    "            V[-i-1] = np.random.randn(hidden_size, hidden_size)*1 # W_hh\n",
    "        U = np.random.randn(hidden_size, input_size)*1            # W_hx\n",
    "        W = np.random.randn(out_size, hidden_size)*1              # W_hy\n",
    "        b_hidden = np.zeros((hidden_size, 1))                        # b_h\n",
    "        b_out = np.zeros((out_size, 1))                              # b_y\n",
    "        \n",
    "        self.params = [V, U, W, b_hidden, b_out]\n",
    "        self.best = [V, U, W, b_hidden, b_out]\n",
    "\n",
    "    def forward(self, inputs, hidden_state, best=False):\n",
    "        if best: \n",
    "            V, U, W, b_hidden, b_out = self.best    \n",
    "        else: \n",
    "            V, U, W, b_hidden, b_out = self.params\n",
    "        outputs, hidden_states = {}, {}\n",
    "        for n in range(len(inputs)):\n",
    "            hidden_states[n] = {}\n",
    "            for i in range(self.order): # for the history window\n",
    "                hidden_states[n][-i-1] = np.copy(hidden_state)\n",
    "            for t in range(len(inputs[n])):\n",
    "                s = np.zeros_like(hidden_state)\n",
    "                for i in range(self.order): # order = 3: -1, -2, -3; \n",
    "                    s += np.dot(V[-i-1], hidden_states[n][t-i-1])\n",
    "                hidden_states[n][t] = np.tanh(np.dot(U, inputs[n][t]) + s + b_hidden)\n",
    "            outputs[n] = sigmoid(np.copy((np.dot(W, hidden_states[n][t]) + b_out).item()))\n",
    "        return outputs, hidden_states\n",
    "    \n",
    "    def backward(self, inputs, outputs, hidden_states, targets, clip_norm):\n",
    "        V, U, W, b_hidden, b_out = self.params\n",
    "        dV = {}\n",
    "        for i in range(self.order):\n",
    "            dV[-i-1] = np.zeros_like(V[-i-1])\n",
    "        dU, dW = np.zeros_like(U), np.zeros_like(W)\n",
    "        db_hidden, db_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "\n",
    "        loss = 0.0\n",
    "        N, T = len(inputs), len(inputs[0])\n",
    "\n",
    "        for n in range(N):\n",
    "            loss += -(targets[n]*np.log(outputs[n]) + (1 - targets[n])*np.log(1 - outputs[n]))\n",
    "            do = outputs[n] - targets[n]\n",
    "            dW += np.dot(do, hidden_states[n][T-1].T)    # uses only last hidden state\n",
    "            db_out += do\n",
    "\n",
    "            # HORNN update\n",
    "            #########################\n",
    "            arrayOfM, arrayOfs = {}, {}\n",
    "            for i in range(self.order):\n",
    "                arrayOfM[i], arrayOfs[i] = 0.0, 0.0\n",
    "\n",
    "            l = np.dot(W.T, do)\n",
    "            one = 1.0\n",
    "            for t in reversed(range(T)):\n",
    "                M = arrayOfM[0]\n",
    "                for i in range(self.order-1):\n",
    "                    M = M * arrayOfs[i] + arrayOfM[i+1]\n",
    "                \n",
    "                M = M * arrayOfs[self.order-1] + one  # M update, one = 1 only at t + T\n",
    "                M *= l                                # l is 1 after t = T\n",
    "                for i in range(self.order-1):\n",
    "                    arrayOfM[i] = arrayOfM[i+1]\n",
    "                arrayOfM[self.order-1] = M\n",
    "\n",
    "                df = (1 - hidden_states[n][t] * hidden_states[n][t])\n",
    "                r = df * M\n",
    "                for i in range(self.order):\n",
    "                    dV[-i-1] += np.dot(r, hidden_states[n][t-i-1].T)\n",
    "                db_hidden += r\n",
    "                dU += np.dot(r, inputs[n][t].T)\n",
    "\n",
    "                for s in range(self.order-1):                   \n",
    "                    arrayOfs[s] = arrayOfs[s+1] \n",
    "                arrayOfs[self.order-1] = np.dot(V[-1].T, df)\n",
    "                \n",
    "                one, l = 0.0, 1.0\n",
    "            #########################\n",
    "\n",
    "        loss /= N\n",
    "        for i in range(self.order):\n",
    "            dV[-i-1] /= N\n",
    "        dU, dW, db_hidden, db_out = dU/N, dW/N, db_hidden/N, db_out/N\n",
    "        grads = [dV, dU, dW, db_hidden, db_out]\n",
    "        grads = gradient_clip(grads, clip_norm, self.order)\n",
    "        return loss, grads\n",
    "    \n",
    "    def train(self, train_set, valid_set, hidden_state, num_epochs, lr, clip_norm, train_steps=100, val_steps=100):\n",
    "        training_loss, validation_loss = [], []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_training_loss = 0.0\n",
    "            epoch_validation_loss = 0.0\n",
    "            min_valid_loss = 1e6\n",
    "\n",
    "            for train_step in range(train_steps):\n",
    "                inputs, targets = next(train_set)\n",
    "                outputs, hidden_states = self.forward(inputs, hidden_state)\n",
    "                loss, grads = self.backward(inputs, outputs, hidden_states, targets, clip_norm)\n",
    "                self.params = update_parameters(self.params, grads, lr, self.order)\n",
    "                epoch_training_loss += loss\n",
    "\n",
    "            for val_step in range(val_steps):\n",
    "                inputs, targets = next(valid_set)\n",
    "                outputs, _ = self.forward(inputs, hidden_state)\n",
    "                loss = find_loss(outputs, targets)\n",
    "                if loss < min_valid_loss:\n",
    "                    self.best = self.params.copy()\n",
    "                    min_valid_loss = loss\n",
    "                epoch_validation_loss += loss\n",
    "\n",
    "            training_loss.append(epoch_training_loss/train_steps)\n",
    "            validation_loss.append(epoch_validation_loss/val_steps)\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                print(f'Epoch {epoch}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "        return training_loss, validation_loss\n",
    "\n",
    "    def test(self, test_set, hidden_state, test_steps=100):\n",
    "        test_loss, test_acc = 0.0, 0.0\n",
    "        for test_step in range(test_steps):\n",
    "            inputs, targets = next(test_set)\n",
    "            outputs, _ = self.forward(inputs, hidden_state, best=True)\n",
    "            test_loss += find_loss(outputs, targets)\n",
    "            test_acc += find_acc(outputs, targets)\n",
    "        test_loss /= test_steps\n",
    "        test_acc /= test_steps\n",
    "        print(\"Test loss: \", test_loss)\n",
    "        print(\"Test acc: \", test_acc)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0fq__WNTAbN"
   },
   "source": [
    "## Train HORNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJCffXX8TDwY"
   },
   "outputs": [],
   "source": [
    "### Train HORNN ###\n",
    "# Hyperparameters\n",
    "hidden_size = 16\n",
    "gradient_clip_norm = 0.1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "num_exp = 5\n",
    "b_size = 64\n",
    "train_steps, val_steps, test_steps = 5, 2, 2\n",
    "orders = [2, 3, 5]\n",
    "seq_lens = [50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41XUj7TNYSTe"
   },
   "outputs": [],
   "source": [
    "hornn_test_results = {}\n",
    "for seq_len in seq_lens:\n",
    "    trainx, trainy = filter(train_data, train_labels, seqlen)\n",
    "    x_train = sequence.pad_sequences(trainx, maxlen=seqlen)\n",
    "    y_train = np.asarray(trainy).astype('float32')\n",
    "\n",
    "    testx, testy = filter(test_data, test_labels, seqlen)\n",
    "    x_test = sequence.pad_sequences(testx, maxlen=seqlen)\n",
    "    y_test = np.asarray(testy).astype('float32')\n",
    "\n",
    "    train_gen = data_generator(x_train[:b_size*train_steps], \n",
    "                        y_train[:b_size*train_steps], batch_size=b_size)\n",
    "    val_gen = data_generator(x_test[:b_size*val_steps], \n",
    "                    y_test[:b_size*val_steps], batch_size=b_size)\n",
    "    test_gen = data_generator(x_test[b_size*val_steps:b_size*(val_steps+test_steps)], \n",
    "                        y_test[b_size*val_steps:b_size*(val_steps+test_steps)], batch_size=b_size)\n",
    "\n",
    "    hornn_test_results[seq_len] = {}\n",
    "    for order in orders:\n",
    "        hornn_test_results[seq_len][order] = {}\n",
    "        test_loss_results, test_acc_results = [], []\n",
    "        for n in range(num_exp):\n",
    "            print(f'HORNN {order} Experiment: {n}; seq_len: {seq_len}')\n",
    "            hornn_model = HORNN(hidden_size=hidden_size, input_size=10000, out_size=1, order=order)\n",
    "            hidden_state = np.zeros((hidden_size, 1))  # Initial hidden_state\n",
    "            hornn_train_loss, hornn_valid_loss = hornn_model.train(train_set=train_gen, valid_set=val_gen, \n",
    "                                                    hidden_state=hidden_state, num_epochs=num_epochs, \n",
    "                                                    lr=learning_rate, clip_norm=gradient_clip_norm, \n",
    "                                                    train_steps=train_steps, val_steps=val_steps)\n",
    "            hornn_test_loss, hornn_test_acc = hornn_model.test(test_set=test_gen, hidden_state=hidden_state, test_steps=test_steps)\n",
    "            test_loss_results.append(test_loss)\n",
    "            test_acc_results.append(test_acc)\n",
    "            # Plot training and validation loss\n",
    "            epoch = np.arange(len(train_loss))\n",
    "            plt.figure()\n",
    "            plt.plot(epoch, train_loss, 'r', label='Training loss',)\n",
    "            plt.plot(epoch, valid_loss, 'b', label='Validation loss')\n",
    "            plt.legend()\n",
    "            plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "            plt.savefig(f'imdb_{seq_len}__hornn{order}_exp_{n}.png')\n",
    "        \n",
    "        print(f'Seq_len: {seq_len}; HORNN-{order} average imdb test loss: {np.average(test_loss_results)}')\n",
    "        print(f'Seq_len: {seq_len}; HORNN-{order} average imdb test acc: {np.average(test_acc_results)}')\n",
    "        hornn_test_results[seq_len][order]['loss'] = np.average(test_loss_results)\n",
    "        hornn_test_results[seq_len][order]['acc'] = np.average(test_acc_results)\n",
    "        hornn_test_results[seq_len][order]['all_loss'] = test_loss_results\n",
    "        hornn_test_results[seq_len][order]['all_acc'] = test_acc_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNoM/twSeCEiRP2Ofwl+fuk",
   "collapsed_sections": [
    "qk8Jgz4HUeg_",
    "vQtbTlKH7vKZ",
    "BIMd8wRj70Vn",
    "p6825TQW74kX"
   ],
   "name": "imdb.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
